{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kernel fastai\n",
    "import pandas as pd\n",
    "import tsfresh as ts\n",
    "import numpy as np\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('E:/Data/OAPMLData/V1.1_CAIC_UAC_NWAC_FeaturesAsTimeSeries20131101To20180430.csv', parse_dates=['__fileDate'], low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_season(row):\n",
    "    year = row['__fileDate'].year\n",
    "    month = row['__fileDate'].month\n",
    "    if year == 2013 or (year == 2014 and month < 5):\n",
    "        return '13-14'\n",
    "    elif (year == 2014 and month > 10) or (year == 2015 and month < 5):\n",
    "        return '14-15'\n",
    "    elif (year == 2015 and month > 10) or (year == 2016 and month < 5):\n",
    "        return '15-16'\n",
    "    elif (year == 2016 and month > 10) or (year == 2017 and month < 5):\n",
    "        return '16-17'\n",
    "    elif (year == 2017 and month > 10) or (year == 2018 and month < 5):\n",
    "        return '17-18'\n",
    "    else:\n",
    "        return 'unknown season'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Season'] = df.apply(identify_season, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean mising values\n",
    "mappingMissingValues = {-9999:np.nan}\n",
    "\n",
    "df = df.replace(mappingMissingValues)\n",
    "filter = df[['Season', 'Lat', 'Lon']].drop_duplicates()\n",
    "\n",
    "listOfFrames = []\n",
    "for f in filter.iterrows():\n",
    "    listOfFrames.append(df[(df['Season'] == f[1]['Season']) & (df['Lat'] == f[1]['Lat']) & (df['Lon'] == f[1]['Lon'])].interpolate(method='linear', axis=0).ffill().bfill())\n",
    "\n",
    "interpolated = pd.concat(listOfFrames)\n",
    "\n",
    "#fill in any remaining nan with 0 as that datapoint was probably missing for a long\n",
    "#period of time (all of the sesason for the lat/lon)\n",
    "\n",
    "interpolated.fillna(0, inplace=True) \n",
    "\n",
    "df=interpolated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['APCPSurface', 'MaxTempSurfaceF',\n",
    "              'MinTempSurfaceF', 'AvgTempSurfaceF', 'MaxTemp2mAboveGroundF',\n",
    "              'MinTemp2mAboveGroundF', 'AvgTemp2mAboveGroundF',\n",
    "              'MaxTemp80mAboveGroundF', 'MinTemp80mAboveGroundF',\n",
    "              'AvgTemp80mAboveGroundF', 'MaxTempTropF', 'MinTempTropF',\n",
    "              'AvgTempTropF', 'AvgRH2mAboveGround', 'AvgWindDirection10m',\n",
    "              'AvgWindDirection80m', 'AvgWindDirectionTrop', 'AvgWindSpeed10m',\n",
    "              'MaxWindSpeed10m', 'AvgWindSpeed80m', 'MaxWindSpeed80m',\n",
    "              'AvgWindSpeedTrop', 'MaxWindSpeedTrop',                \n",
    "              'SnowWaterEquivalentIn', 'PrecipIncrementSnowIn',\n",
    "              'PrecipitationAccumulation', 'SnowDepthIn', 'TempMinF', 'TempMaxF',\n",
    "              'TempAveF', 'SNOWDAS_SnowDepth_mm', 'SNOWDAS_SWE_mm',\n",
    "              'SNOWDAS_SnowmeltRunoff_micromm', 'SNOWDAS_Sublimation_micromm',\n",
    "              'SNOWDAS_SublimationBlowing_micromm',\n",
    "              'SNOWDAS_SolidPrecip_kgpersquarem',\n",
    "              'SNOWDAS_LiquidPrecip_kgpersquarem', 'SNOWDAS_SnowpackAveTemp_k',\n",
    "              'c_IsCoastalSnowpack', 'c_IsContenentalSnowpack']\n",
    "output_cols = ['o_Day1DangerAboveTreeline', 'o_Day1DangerNearTreeline', 'o_Day1DangerBelowTreeline']\n",
    "metadata_cols = ['Lat', 'Lon', '__fileDate', 'c_IsCoastalSnowpack', 'c_IsContenentalSnowpack', 'UnifiedRegion']\n",
    "\n",
    "df_X = df[input_cols]\n",
    "df_y = df[output_cols + metadata_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X['id'] = (df['Lat']*100000).astype(int).apply(str) + (df['Lon']*-100000).astype(int).apply(str) + df['Season']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X['date'] = df['__fileDate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df_X.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = df_y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y.index = df_X['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X[df_X['id']=='45325031217681113-14'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.to_pickle('E:/Temp/ts_df_X.pkl')\n",
    "df_y.to_pickle('E:/Temp/ts_df_y.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = pd.read_pickle('E:/Temp/ts_df_X.pkl')\n",
    "df_y = pd.read_pickle('E:/Temp/ts_df_y.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n",
    "unique_ids = df_X['id'].unique()\n",
    "tmp = ts.utilities.dataframe_functions.roll_time_series(df_X[df_X['id']==unique_ids[0]], column_id='id', column_sort='date', column_kind=None, rolling_direction=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n",
    "tmp_y = df_y[df_y.index==unique_ids[0]]\n",
    "tmp_y.index = tmp_y['__fileDate'].astype(str) + '-' + unique_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.to_csv(\"testroll.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = df_X['id'].unique()\n",
    "result_x = []\n",
    "result_y = []\n",
    "count = 0\n",
    "\n",
    "for i in unique_ids[count:1]:\n",
    "    print('Iteration: ' + str(count) + ' of ' + str(len(unique_ids)))\n",
    "    count += 1\n",
    "    tmp = ts.utilities.dataframe_functions.roll_time_series(df_X[df_X['id']==i], column_id='id', column_sort='date', column_kind=None, rolling_direction=1)\n",
    "    tmp_ext = ts.extract_features(tmp, column_id='id', column_sort='date', n_jobs=8, chunksize=400, show_warnings=False)\n",
    "    tmp_ext.index = pd.Series(tmp_ext.index.format()) + '-' + i\n",
    "    tmp_y = df_y[df_y.index==i]\n",
    "    tmp_y.index = tmp_y['__fileDate'].astype(str) + '-' + i\n",
    "    tmp_ext.to_pickle('E:/Temp/time-series/ext_' + i + '.pkl')\n",
    "    tmp_y.to_pickle('E:/Temp/time-series/y_' + i + '.pkl')\n",
    "    #result_x.append(tmp_ext)\n",
    "    #result_y.append(tmp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_impute(f):\n",
    "    try:  \n",
    "        return ts.utilities.dataframe_functions.impute(pd.read_pickle('E:/Temp/time-series/' + f))\n",
    "    except:\n",
    "        print('Exception on file ' + f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from joblib import Parallel, delayed\n",
    "from os import listdir\n",
    "\n",
    "file = ['13-14.pkl', '14-15.pkl', '15-16.pkl', '16-17.pkl', '17-18.pkl']\n",
    "files = []\n",
    "i = 4\n",
    "#for i in range(0, len(file)):\n",
    "ext_files = [f for f in listdir('E:/Temp/time-series/') if f.startswith('ext_') and (f.endswith(file[i]))]\n",
    "files = Parallel(n_jobs=10)(delayed(read_and_impute)(f) for f in ext_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_df = pd.concat(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_files = [f for f in listdir('E:/Temp/time-series/') if f.startswith('y_')  and (f.endswith(file[i]))]\n",
    "yfiles = []\n",
    "for f in y_files:\n",
    "    try:\n",
    "        yfiles.append(pd.read_pickle('E:/Temp/time-series/' + f))\n",
    "    except:\n",
    "        print('Exception on file ' + f)\n",
    "\n",
    "y_df = pd.concat(yfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#TODO: convert to DASK\n",
    "#from os import listdir\n",
    "#year = ['14', '15', '16', '17', '18']\n",
    "#file = ['13-14.pkl', '14-15.pkl', '15-16.pkl', '16-17.pkl', '17-18.pkl']\n",
    "\n",
    "#i = 0\n",
    "#for i in range(0, len(file)):\n",
    "#ext_files = [f for f in listdir('E:/Temp/time-series/') if f.startswith('ext_') and (f.endswith(file[i]))] # or f.endswith('16.pkl'))]\n",
    "#y_files = [f for f in listdir('E:/Temp/time-series/') if f.startswith('y_')  and (f.endswith(file[i]))] # or f.endswith('16.pkl'))]\n",
    "#files = []\n",
    "#for f in ext_files:\n",
    "#    try:        \n",
    "#        files.append(ts.utilities.dataframe_functions.impute(pd.read_pickle('E:/Temp/time-series/' + f)))\n",
    "#    except Exception:\n",
    "#        print('Exception on file ' + f)\n",
    "\n",
    "#ext_df = pd.concat(files)\n",
    "#yfiles = []\n",
    "#for f in y_files:\n",
    "#    yfiles.append(pd.read_pickle('E:/Temp/time-series/' + f))\n",
    "\n",
    "#y_df = pd.concat(yfiles)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ext_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df['o_Day1DangerAboveTreeline'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del yfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_df.sort_index(inplace=True)\n",
    "y_df.sort_index(inplace=True)\n",
    "\n",
    "y_df = y_df[y_df['o_Day1DangerAboveTreeline']!='Extreme']\n",
    "y_df = y_df[y_df['o_Day1DangerAboveTreeline']!='no-data']\n",
    "y_df = y_df[y_df['o_Day1DangerAboveTreeline']!=0]\n",
    "y_df = y_df['o_Day1DangerAboveTreeline'].dropna()\n",
    "ext_df = ext_df.reindex(y_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_df.to_parquet('E:\\Temp\\df_rolled_x_17-18.par')\n",
    "y_df.to_pickle('E:\\Temp\\df_rolled_y_17-18.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_df = pd.read_parquet('E:\\Temp\\df_rolled_x_14-15.par')\n",
    "y_df = pd.read_pickle('E:\\Temp\\df_rolled_y_14-15.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_df_sampled = y_df.sample(frac=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del ext_df_samples\n",
    "#del ext_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_resampled, y_resampled = rus.fit_resample(ext_df, y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_resampled).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ext_df_samples = ext_df.reindex(y_df_sampled.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled = pd.DataFrame(X_resampled, columns=ext_df.columns)\n",
    "#y_resampled = pd.Series(y_resampled, ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_df_filtered = ts.select_features(X_resampled, y_resampled, n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(ext_df_filtered.columns).to_csv(\"E:/Temp/17-18tsfreshfeatrues.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['13-14tsfreshfeatures.csv','14-15tsfreshfeatures.csv','15-16tsfreshfeatures.csv','16-17tsfreshfeatures.csv','17-18tsfreshfeatures.csv']\n",
    "dfs = []\n",
    "for f in files:\n",
    "    dfs.append(pd.read_csv('E:/Temp/' + f, header=None)[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = dfs.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_X = ['df_rolled_x_13-14.par','df_rolled_x_14-15.par','df_rolled_x_15-16.par','df_rolled_x_16-17.par','df_rolled_x_17-18.par']\n",
    "\n",
    "files_y = ['df_rolled_y_13-14.pkl','df_rolled_y_14-15.pkl','df_rolled_y_15-16.pkl','df_rolled_y_16-17.pkl','df_rolled_y_17-18.pkl']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "#X_resampled, y_resampled = rus.fit_resample(ext_df, y_df)\n",
    "\n",
    "dfs_X = []\n",
    "dfs_y = []\n",
    "\n",
    "#for f_i in range(0,len(files_X)):\n",
    "f_i = 0\n",
    "tmp_X = pd.read_parquet('E:/Temp/' + files_X[f_i], columns=columns)\n",
    "tmp_y = pd.read_pickle('E:/Temp/' + files_y[f_i])\n",
    "tmp_X_resampled, tmp_y_resampled = rus.fit_resample(tmp_X, tmp_y)\n",
    "dfs_X.append(tmp_X_resampled)\n",
    "dfs_y.append(tmp_y_resampled)\n",
    "del tmp_X\n",
    "del tmp_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_i = 3\n",
    "tmp_X = pd.read_parquet('E:/Temp/' + files_X[f_i], columns=columns)\n",
    "tmp_y = pd.read_pickle('E:/Temp/' + files_y[f_i])\n",
    "tmp_X_resampled, tmp_y_resampled = rus.fit_resample(tmp_X, tmp_y)\n",
    "dfs_X.append(tmp_X_resampled)\n",
    "dfs_y.append(tmp_y_resampled)\n",
    "del tmp_X\n",
    "del tmp_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs_X = [pd.DataFrame(i, columns=columns) for i in dfs_X]\n",
    "pdfs_y = [pd.Series(i) for i in dfs_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_filtered = pd.concat(pdfs_X)\n",
    "df_rolled_y_sampled2 = pd.concat(pdfs_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolled_y_sampled2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_i = 4\n",
    "df_rolled_x_sampled_test = pd.read_parquet('E:/Temp/' + files_X[f_i], columns=columns)\n",
    "df_rolled_y_sampled2_test = pd.read_pickle('E:/Temp/' + files_y[f_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rolled_x.to_pickle('E:\\Temp\\df_rolled_x2016.pkl')\n",
    "#df_rolled_y.to_pickle('E:\\Temp\\df_rolled_y2016.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rolled_x_sampled = df_rolled_x.sample(frac=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rolled_x_sampled_test = df_rolled_x.sample(frac=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rolled_x_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rolled_y_sampled_test = df_rolled_y.reindex(df_rolled_x_sampled_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del df_rolled_x\n",
    "#del df_rolled_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del ext_df\n",
    "#del y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp = []\n",
    "#num_cols = 1000\n",
    "#for i in range(0, int(len(df_rolled_x.columns)), num_cols):\n",
    "#    impute_me = df_rolled_x.iloc[:, i:i+num_cols].copy()\n",
    "    \n",
    "#    tmp.append(impute_me.reindex(df_rolled_y_sampled2_test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del df_rolled_x\n",
    "#del ext_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp2 = pd.concat(tmp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_x = ['df_rolled_x_13-14.pkl','df_rolled_x_14-15.pkl','df_rolled_x_15-16.pkl', 'df_rolled_x_16-17.pkl']\n",
    "files_y = ['df_rolled_y_13-14.pkl','df_rolled_y_14-15.pkl','df_rolled_y_15-16.pkl','df_rolled_y_16-17.pkl']\n",
    "files_array_x = []\n",
    "files_array_y = []\n",
    "for f in files_x:\n",
    "    files_array_x.append(pd.read_pickle('E:/Temp/' + f ))\n",
    "    \n",
    "for f in files_y:\n",
    "    files_array_y.append(pd.read_pickle('E:/Temp/' + f ))\n",
    "\n",
    "df_rolled_x_sampled = pd.concat(files_array_x)\n",
    "df_rolled_y_sampled2 = pd.concat(files_array_y)\n",
    "\n",
    "\n",
    "df_rolled_x_sampled_test = pd.read_pickle('E:/Temp/df_rolled_x_17-18.pkl')\n",
    "df_rolled_y_sampled2_test = pd.read_pickle('E:/Temp/df_rolled_y_17-18.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del files_array_x\n",
    "del files_array_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolled_x_sampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolled_y_sampled2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolled_x_sampled_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolled_y_sampled2_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_rolled_x_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_filtered.to_pickle('E:\\Temp\\df_rolled_x_selected_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "m = RandomForestClassifier(n_jobs = 4,\n",
    "                           #oob_score=True,\n",
    "                           n_estimators=50,\n",
    "                           #max_features=\"sqrt\",\n",
    "                           min_samples_leaf=100\n",
    "                           )\n",
    "m.fit(tmp_filtered.values, df_rolled_y_sampled2.values.ravel())\n",
    "m.score(tmp_filtered.values, df_rolled_y_sampled2.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.score(df_rolled_x_sampled_test.values, df_rolled_y_sampled2_test.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import error_evaluation\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "preds = m.predict(df_rolled_x_sampled_test.values)\n",
    "\n",
    "error_evaluation.evaluateSingleClassShort(df_rolled_y_sampled2_test, preds)\n",
    "cnf_matrix = confusion_matrix(df_rolled_y_sampled2_test, preds)\n",
    "plt.figure()\n",
    "error_evaluation.plot_confusion_matrix(cnf_matrix, classes=['Low', 'Moderate', 'Considerable', 'High'], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.figure()\n",
    "error_evaluation.plot_confusion_matrix(cnf_matrix, classes=['Low', 'Moderate', 'Considerable', 'High'],\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = m.predict(tmp_filtered.values)\n",
    "\n",
    "error_evaluation.evaluateSingleClassShort(df_rolled_y_sampled2, preds)\n",
    "cnf_matrix = confusion_matrix(df_rolled_y_sampled2, preds)\n",
    "plt.figure()\n",
    "error_evaluation.plot_confusion_matrix(cnf_matrix, classes=['Low', 'Moderate', 'Considerable', 'High'], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.figure()\n",
    "error_evaluation.plot_confusion_matrix(cnf_matrix, classes=['Low', 'Moderate', 'Considerable', 'High'],\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(m.feature_importances_)\n",
    "out = pd.concat([pd.Series(tmp_filtered.columns), importances], axis=1)\n",
    "out.columns=['Feature','Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = out.sort_values(by=['Score'], ascending=False)[:1000]['Feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(important_features).to_csv('E:/Temp/top1000tsfreshfeatures.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = BalancedRandomForestClassifier(n_jobs = 4,\n",
    "                           #oob_score=True,\n",
    "                           n_estimators=50,\n",
    "                           #max_features=\"sqrt\",\n",
    "                           min_samples_leaf=100\n",
    "                           )\n",
    "m.fit(tmp_filtered[important_features].values, df_rolled_y_sampled2.values.ravel())\n",
    "m.score(tmp_filtered[important_features].values, df_rolled_y_sampled2.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = m.predict(df_rolled_x_sampled_test[important_features].values)\n",
    "\n",
    "error_evaluation.evaluateSingleClassShort(df_rolled_y_sampled2_test, preds)\n",
    "cnf_matrix = confusion_matrix(df_rolled_y_sampled2_test, preds)\n",
    "plt.figure()\n",
    "error_evaluation.plot_confusion_matrix(cnf_matrix, classes=['Low', 'Moderate', 'Considerable', 'High'], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.figure()\n",
    "error_evaluation.plot_confusion_matrix(cnf_matrix, classes=['Low', 'Moderate', 'Considerable', 'High'],\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-360cf8729bee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mDataPrep\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0merror_evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataPrep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataPrep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_Above_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_Above_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_Above_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_Above_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_Near_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_Near_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_Near_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_Near_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_Below_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_Below_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_Below_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_Below_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprep_day1_danger_train_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'E:/Data/OAPMLData/V1.1CIAC_UAC_NWAC_FeaturesWithLabels30Days20131201To20180430.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_extreme\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monly_critical_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moversample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monly_precise_points\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_critical_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\src\\GitHub\\OpenAvalancheProject\\ML\\DataPrep.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshapely\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshapely\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import DataPrep\n",
    "import error_evaluation\n",
    "import pandas as pd\n",
    "dp = DataPrep.DataPrep()\n",
    "X_Above_test, X_Above_train, y_Above_test, y_Above_train, X_Near_test, X_Near_train, y_Near_test, y_Near_train, X_Below_test, X_Below_train, y_Below_test, y_Below_train = dp.prep_day1_danger_train_test(input_file='E:/Data/OAPMLData/V1.1CIAC_UAC_NWAC_FeaturesWithLabels30Days20131201To20180430.csv', ignore_extreme=True, only_critical_points=False, oversample=False, only_precise_points = False, label_critical_points=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
